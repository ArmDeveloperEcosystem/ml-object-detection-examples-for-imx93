{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DetectNet V2 Model Conversion\n",
        "\n",
        "```\n",
        "SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>\n",
        "SPDX-License-Identifier: MIT\n",
        "```\n",
        "\n",
        "This notebook can be used to convert a pre-trained DetectNet V2 model that is in ONNX format to TensorFlow Lite format to run on an Arm Cortex-A CPU or Arm Ethos-U NPU on a Linux based IoT platform."
      ],
      "metadata": {
        "id": "nhlgDkQFYulR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload ONNX model\n",
        "\n",
        "Place the ONNX model in the notebook's directory or upload it the the Google Colab instance. Then update the variable value below with the model's filename."
      ],
      "metadata": {
        "id": "8HtGbvbKZMvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "INPUT_MODEL = \"model_name.onnx\"\n",
        "\n",
        "INPUT_MODEL_ROOT, INPUT_MODEL_EXT = os.path.splitext(INPUT_MODEL)"
      ],
      "metadata": {
        "id": "v97ZFcmcZklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename inputs and outputs using ONNX.\n",
        "\n",
        "This section will remove the `:0` suffixes from the model's inputs and outputs."
      ],
      "metadata": {
        "id": "H_P1RaHXZ3gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install ONNX"
      ],
      "metadata": {
        "id": "iyy-7xQ4aJac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "L2_BIa89AYMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the ONNX library to create a modfied model with the renamed inputs and outputs."
      ],
      "metadata": {
        "id": "ZADgvwtSaMir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(INPUT_MODEL)\n",
        "\n",
        "# input and output names to remove :0 suffix from\n",
        "suffix = ':0'\n",
        "\n",
        "graph_input_names = [input.name for input in onnx_model.graph.input]\n",
        "graph_output_names = [output.name for output in onnx_model.graph.output]\n",
        "\n",
        "print('graph_input_names =', graph_input_names)\n",
        "print('graph_output_names =', graph_output_names)\n",
        "\n",
        "for input in onnx_model.graph.input:\n",
        "\tinput.name = input.name.removesuffix(suffix)\n",
        "\n",
        "for output in onnx_model.graph.output:\n",
        "\toutput.name = output.name.removesuffix(suffix)\n",
        "\n",
        "for node in onnx_model.graph.node:\n",
        "\tfor i in range(len(node.input)):\n",
        "\t\tif node.input[i] in graph_input_names:\n",
        "\t\t\tnode.input[i] = node.input[i].removesuffix(suffix)\n",
        "\n",
        "\tfor i in range(len(node.output)):\n",
        "\t\tif node.output[i] in graph_output_names:\n",
        "\t\t\tnode.output[i] = node.output[i].removesuffix(suffix)\n",
        "\n",
        "MODIFIED_ONNX_MODEL = f'{INPUT_MODEL_ROOT}_mod{INPUT_MODEL_EXT}'\n",
        "onnx.save(onnx_model, MODIFIED_ONNX_MODEL)"
      ],
      "metadata": {
        "id": "izO4fWzwAjtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert model to OpenVINO format"
      ],
      "metadata": {
        "id": "xgwJbYUjbi7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install OpenVINO"
      ],
      "metadata": {
        "id": "qmmrsGqNb117"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openvino_dev"
      ],
      "metadata": {
        "id": "vU8BNVopDaUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `mo` command to convert ONNX model to OpenVINO format"
      ],
      "metadata": {
        "id": "MmC6MFAWb5bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mo \\\n",
        "  --input_model {MODIFIED_ONNX_MODEL} \\\n",
        "  --input_shape [1,3,544,960] \\\n",
        "  --output_dir {INPUT_MODEL_ROOT}_openvino \\\n",
        "  --compress_to_fp16=False"
      ],
      "metadata": {
        "id": "3CCQeZZgDgF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert OpenVINO model to TensorFlow"
      ],
      "metadata": {
        "id": "oxCM9ZyNcqpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `openvino2tensorflow`"
      ],
      "metadata": {
        "id": "q-BGUCSPc0YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openvino2tensorflow"
      ],
      "metadata": {
        "id": "wsLRTT1WDsW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `openvino2tensorflow` command to convert OpenVINO model. This will change the model from NCHW to NHWC format."
      ],
      "metadata": {
        "id": "bW4OUGELc5fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openvino2tensorflow \\\n",
        "  --model_path {INPUT_MODEL_ROOT}_openvino/{INPUT_MODEL_ROOT}_mod.xml \\\n",
        "  --model_output_path {INPUT_MODEL_ROOT}_tensorflow \\\n",
        "  --non_verbose \\\n",
        "  --output_saved_model"
      ],
      "metadata": {
        "id": "dj0YBvBlD0O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert TensorFlow model to TensorFlow Lite format\n"
      ],
      "metadata": {
        "id": "nU1X1lWcdNc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install TensorFlow"
      ],
      "metadata": {
        "id": "Q-V-wjKjgKAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "nx-CaUQbgMrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use TensorFlow to quantize the model with random representative data and convert to TensorFlow Lite format.\n"
      ],
      "metadata": {
        "id": "xKBny6WUgUZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(f'{INPUT_MODEL_ROOT}_tensorflow')\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "def representative_dataset():\n",
        "  for _ in range(10):\n",
        "    yield [\n",
        "        tf.random.uniform((1, 544, 960, 3))\n",
        "    ]\n",
        "\n",
        "converter.optimizations = [\n",
        "    tf.lite.Optimize.DEFAULT\n",
        "]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
        "]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.float32\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "with open(f'{INPUT_MODEL_ROOT}.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n"
      ],
      "metadata": {
        "id": "EmEK6-vLFmuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the TensorFlow Lite model with Vela"
      ],
      "metadata": {
        "id": "0Sq5cvunegXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the `vela` compiler"
      ],
      "metadata": {
        "id": "W6S7JAbSepEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ethos-u-vela"
      ],
      "metadata": {
        "id": "_I4WBFjOr0tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the quantized TensorFlow Lite model"
      ],
      "metadata": {
        "id": "Qcq-7xfbeudU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!vela \\\n",
        "  --config Arm/vela.ini \\\n",
        "  --accelerator-config ethos-u65-256 \\\n",
        "  --system-config Ethos_U65_High_End \\\n",
        "  --memory-mode Dedicated_Sram \\\n",
        "  --output-dir . \\\n",
        "  {INPUT_MODEL_ROOT}.tflite"
      ],
      "metadata": {
        "id": "FbRRTyeoz0YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}